# -*- coding: utf-8 -*-
"""Fake News Detection

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rROLgKFiKn6KdjkGzNVGM9YAEJojC2QS
"""

# Install nltk and download stopwords
!pip install -q nltk

import pandas as pd
import numpy as np
import string
import nltk
import matplotlib.pyplot as plt
import seaborn as sns
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from sklearn.pipeline import Pipeline
import joblib
import itertools

nltk.download('stopwords')
stop_words = set(stopwords.words('english'))

# Load datasets
fake = pd.read_csv("/content/Fake.csv")
true = pd.read_csv("/content/True.csv")

# Add target labels
fake['target'] = 'fake'
true['target'] = 'true'

# Display dataset info
print("\nFake news dataset shape:", fake.shape)
print("\nTrue news dataset shape:", true.shape)

fake.count()

true.count()

print("\nFake news sample(head):")
fake.head(5)

print("\nFake news sample(tail):")
fake.tail(5)

print("\nTrue news sample(head):")
true.head(5)

print("\nTrue news sample(tail):")
true.tail(5)

# Find Missing Data if any at all
def find_missing_vals(data):
    total = len(data)
    for column in data.columns:
        if data[column].isna().sum() != 0:
            print(f"{column} has: {data[column].isna().sum()} ({(data[column].isna().sum()/total)*100:.2f}%) missing values.")
        else:
            print(f"{column} has no missing values.")
    print("\nMissing Value Summary\n" + "-"*35)
    print("\nDataframe Missing Values\n" + "-"*35)
    print(data.isnull().sum(axis=0))

find_missing_vals(fake)

find_missing_vals(true)

def remove_duplicates(data):
  print("\nCleaning Summary\n{}".format("-"*35))
  size_before = len(data)
  data.drop_duplicates(subset=None, keep="first", inplace=True)
  size_after= len(data)
  print("Removed {} duplicate rows in db data".format(size_before-size_after))

remove_duplicates(fake)

remove_duplicates(true)

# Combine datasets
data_merged = pd.concat([fake, true]).reset_index(drop=True)
from sklearn.utils import shuffle
data_merged = shuffle(data_merged).reset_index(drop=True)

# Drop unnecessary columns
data_merged.drop(["date", "title"], axis=1, inplace=True)

# Lowercase text
data_merged['text'] = data_merged['text'].apply(lambda x: x.lower())

# Remove punctuation
def remove_punctuation(text):
    return text.translate(str.maketrans('', '', string.punctuation))

data_merged['text'] = data_merged['text'].apply(remove_punctuation)

# Remove stopwords
def remove_stopwords(text):
    return ' '.join([word for word in text.split() if word not in stop_words])

data_merged['text'] = data_merged['text'].apply(remove_stopwords)

print(f"\nData shape after loading and initial cleaning: {data_merged.shape}")
data_merged.head()

print(data_merged.groupby(['subject'])['text'].count())
# Group by subject and count the number of articles
article_counts = data_merged.groupby(['subject'])['text'].count()
plt.figure(figsize=(10, 6))
# Plot bar chart
ax = article_counts.plot(kind="bar")
plt.title("Articles per Subject")
plt.ylabel("Number of Articles")
plt.xlabel("Subject")

# Annotate each bar with the count
for i, count in enumerate(article_counts):
    ax.text(i, count + 1, str(count), ha='center', va='bottom')

plt.tight_layout()
plt.show()

print(data_merged.groupby(['target'])['text'].count())
# Group by target and count the number of articles
target_counts = data_merged.groupby(['target'])['text'].count()
plt.figure(figsize=(5, 5))
# Plot bar chart
ax = target_counts.plot(kind="bar")
plt.title("Fake vs Real Articles")
plt.ylabel("Number of Articles")

# Annotate each bar with the count
for i, count in enumerate(target_counts):
    ax.text(i, count + 1, str(count), ha='center', va='bottom')

plt.tight_layout()
plt.show()

from nltk import tokenize
token_space = tokenize.WhitespaceTokenizer()

def counter(text, column_text, quantity):
    all_words = ' '.join([text for text in text[column_text]])
    token_phrase = token_space.tokenize(all_words)
    frequency = nltk.FreqDist(token_phrase)
    df_frequency = pd.DataFrame({"Word": list(frequency.keys()), "Frequency": list(frequency.values())})
    df_frequency = df_frequency.nlargest(columns="Frequency", n=quantity)
    plt.figure(figsize=(12,8))
    ax = sns.barplot(data=df_frequency, x="Word", y="Frequency")
    ax.set(ylabel="Count")
    plt.xticks(rotation='vertical')
    plt.show()

# Most frequent words in fake news
print("\nMost frequent Words in Fake News\n")
counter(data_merged[data_merged["target"] == "fake"], "text", 20)

# Most frequent words in real news
print("\nMost frequent Words in True News\n")
counter(data_merged[data_merged["target"] == "true"], "text", 20)

# Split data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(data_merged['text'], data_merged['target'], test_size=0.2, random_state=42)
# Create pipeline with TF-IDF and Logistic Regression
pipe = Pipeline([
    ('tfidf', TfidfVectorizer()),  # Convert text to TF-IDF vectors
    ('model', LogisticRegression())  # Train Logistic Regression model
])

# Fit the model
model = pipe.fit(X_train, y_train)

# Calculate training accuracy
train_prediction = model.predict(X_train)
train_accuracy = accuracy_score(y_train, train_prediction)
print("Training Accuracy: {}%".format(round(train_accuracy * 100, 2)))

# Predict and evaluate accuracy
prediction = model.predict(X_test)
accuracy = accuracy_score(y_test, prediction)
print("Accuracy: {}%".format(round(accuracy * 100, 2)))

# Classification report (text output)
report = classification_report(y_test, prediction)

# Convert classification report to DataFrame
report_dict = classification_report(y_test, prediction, output_dict=True)
report_df = pd.DataFrame(report_dict).transpose()

# Plot classification report as heatmap
plt.figure(figsize=(10, 6))
sns.heatmap(report_df.iloc[:-1, :-1], annot=True, cmap="YlGnBu", fmt=".2f")
plt.title("Classification Report")
plt.ylabel("Classes")
plt.xlabel("Metrics")
plt.show()

cm = confusion_matrix(y_test, prediction)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Fake', 'True'], yticklabels=['Fake', 'True'])
plt.title("Confusion Matrix")
plt.xlabel("Predicted label")
plt.ylabel("True label")
plt.show()

joblib.dump(model, 'fake_news_model.pkl')
print("Model saved.")

# # Load the saved model
# model = joblib.load('/content/fake_news_model.pkl')

# # Get user input
# headline = input("Enter a news headline: ")

# # Make prediction
# if headline:
#     prediction = model.predict([headline])[0]
#     prob_fake = model.predict_proba([headline])[0][0]
#     prob_true = model.predict_proba([headline])[0][1]
#     confidence = prob_fake if prediction == 'fake' else prob_true

#     print(f"Prediction: {prediction.upper()}")
#     print(f"Confidence: {round(confidence * 100, 2)}%")
# else:
#     print("No headline was entered.")

# # Import necessary libraries
# import os
# import pandas as pd
# import joblib
# from google.colab import files
# import string
# from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS
# from IPython.display import display

# # Load the trained model
# model = joblib.load('/content/fake_news_model.pkl')

# # Upload CSV file from your device
# print("Please upload a CSV file")
# uploaded = files.upload()  # Opens file picker dialog in Colab

# # Get the original filename
# original_filename = next(iter(uploaded))

# # Define a fixed filename to rename the uploaded file
# fixed_filename = "uploaded_news.csv"
# os.rename(original_filename, fixed_filename)

# # Load the CSV into a DataFrame
# df = pd.read_csv(fixed_filename)

# # Check if 'text' column exists
# if 'text' in df.columns:

#     # Define text preprocessing functions
#     def remove_punctuation(text):
#         return text.translate(str.maketrans('', '', string.punctuation))

#     def remove_stopwords(text):
#         words = text.split()
#         filtered_words = [word for word in words if word not in ENGLISH_STOP_WORDS]
#         return ' '.join(filtered_words)

#     def preprocess_text(text):
#         text = str(text).lower()
#         text = remove_punctuation(text)
#         text = remove_stopwords(text)
#         return text

#     # Apply preprocessing
#     df['processed_text'] = df['text'].apply(preprocess_text)

#     # Predict using the model (assumes pipeline handles vectorization)
#     predictions = model.predict(df['processed_text'])
#     probs = model.predict_proba(df['processed_text'])

#     # Map predictions to readable labels
#     label_map = {'fake': 'FAKE', 'true': 'REAL'}
#     df['prediction'] = [label_map.get(pred.lower(), pred.upper()) for pred in predictions]
#     df['confidence'] = probs.max(axis=1) * 100  # Confidence as percentage

#     # Display results
#     display(df[['text', 'prediction', 'confidence']])

#     # Save and download results
#     output_file = "prediction_results.csv"
#     df.to_csv(output_file, index=False)
#     print(f"\nResults saved to: {output_file}")
#     files.download(output_file)

# else:
#     print("The uploaded CSV file does not contain a 'text' column.")

!pip install streamlit pyngrok

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import streamlit as st
# import pandas as pd
# import string
# import joblib
# from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS
# 
# # Load the trained model
# model = joblib.load('/content/fake_news_model.pkl')
# 
# st.title("üì∞ Fake News Detector")
# st.write("This app detects if a news headline is fake or real using machine learning.")
# 
# # ---------- Helper Functions ---------- #
# def remove_punctuation(text):
#     return text.translate(str.maketrans('', '', string.punctuation))
# 
# def remove_stopwords(text):
#     words = text.split()
#     filtered_words = [word for word in words if word not in ENGLISH_STOP_WORDS]
#     return ' '.join(filtered_words)
# 
# def preprocess_text(text):
#     text = str(text).lower()
#     text = remove_punctuation(text)
#     text = remove_stopwords(text)
#     return text
# 
# # Single Headline Input
# st.header("üîç Single Headline Check")
# headline = st.text_input("Enter a news headline:")
# 
# if headline:
#     processed_headline = preprocess_text(headline)
#     prediction = model.predict([processed_headline])[0]
#     prob = model.predict_proba([processed_headline])[0]
#     confidence = max(prob)
# 
#     st.markdown(f"**Prediction:** `{prediction.upper()}`")
#     st.markdown(f"**Confidence:** `{round(confidence * 100, 2)}%`")
# 
# # Batch CSV Upload
# st.header("üìÅ Batch Prediction from CSV")
# uploaded_file = st.file_uploader("Upload CSV (must contain a 'text' column)", type="csv")
# 
# if uploaded_file:
#     df = pd.read_csv(uploaded_file)
# 
#     if 'text' in df.columns:
#         df['processed_text'] = df['text'].astype(str).apply(preprocess_text)
#         predictions = model.predict(df['processed_text'])
#         probs = model.predict_proba(df['processed_text'])
# 
#         df['prediction'] = [p.upper() for p in predictions]
#         df['confidence'] = (probs.max(axis=1) * 100).round(2)
# 
#         st.success("Prediction completed!")
#         st.dataframe(df[['text', 'prediction', 'confidence']])
# 
#         # CSV download button
#         csv = df.to_csv(index=False)
#         st.download_button(
#             label="üì• Download Results as CSV",
#             data=csv,
#             file_name="fake_news_predictions.csv",
#             mime="text/csv"
#         )
#     else:
#         st.error("Uploaded CSV does not contain a 'text' column.")
#

!pip install pyngrok flask
!pip install flask-ngrok pyngrok
from pyngrok import ngrok

from flask_ngrok import run_with_ngrok
from pyngrok import ngrok
import subprocess

# Set up ngrok with your authtoken
# Replace 'YOUR_NGROK_AUTH_TOKEN' with your actual ngrok authtoken
!ngrok authtoken "30zyBQBi3tMl2gl7XtkNGhdCcxN_6w3p3X1dKe42WBKvygc3C"

# Start Streamlit server in the background
process = subprocess.Popen(['streamlit', 'run', 'app.py', '--server.port', '5000'])

# Create a public URL with ngrok
public_url = ngrok.connect(5000)
print(f"\nStreamlit app running at: {public_url}")